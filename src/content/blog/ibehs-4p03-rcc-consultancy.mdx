---
title: "Real Stakeholders in Course Projects: Interesting in Theory, But..."
summary: "An updated retrospective at school courses and applicability to real life."
publishedAt: 2025-11-30
lastUpdatedAt: 2025-12-29
tags: ['blog', 'university', 'consulting']
---

# Real Stakeholders in Course Projects: Interesting in Theory, But...

> **Update (2025-12-29):** This blog post was originally a submission for an assignment. I've modified it to turn it into a regular blog post without the unnecessary rubric requirements (who puts figure captions in a blog post?). I also *might've phrased things a little too nicely* the first time around.

Hi, it's been a while since I've been able to write another blog post. A little funny since the entire point of me redesigning my homepage was to get me to write more consistently, but things tend to get busy during this time of the year.

Speaking of busy, one thing that has really kept me on my toes is this course I've been taking this fall semester at McMaster: **IBEHS 4P03 - Health Solutions Design Projects IV.** The `XP##` courses are these project-based courses, of which I take one each year, and they're meant for us to learn about and apply non-technical engineering skills while collaborating with other students in different engineering streams.

This year's course focused on project management and consultancy with the project focused around the **Rehabilitation Centre for Children (RCC)**[^1], a non-profit organization based in Winnipeg, Manitoba whose mission is to:

> [...] supports children and youth in discovering their potential and engaging in their communities, through excellence in innovative clinical services, assistive technologies, education, and research.

Essentially, they help children with different mental and physical needs through a combination of physiotherapy, occupational therapy, and specialized equipment that they produce/modify and test in-house. For instance, they might adapt toys with custom switches more readily accessible by a child, like below:

![A slide from their deck, showing children's toys with modified switches.](/blog/ibehs-4p03-rcc-consultancy/slide.png)

We were given problem statements from the RCC, from which our assigned groups were to research, develop, and propose a solution, acting as their consultants.

I initially thought that this would be an interesting course, since we were given the opportunity to bring theory outside of the classroom and apply it with a real stakeholder, potentially enacting some positive change.

However, I ended up rather disappointed with the course as a whole, and especially disappointed with the final solution we came up with. It was arguably the *best* we could muster given the specific circumstances we were put in, but it annoys me how we could've avoided selling a white elephant to a non-profit altogether.

## A solution in search of a problem...

So, here's the specific scope we were given, a few points removed here and there for brevity:

> The electronics department [...] works with a wide range of technologies, each powered by different types of batteries. Device **failures** [...] traced to **faulty or depleted batteries** [...] power sources are **bulky, heavy, and difficult to transport** [... ] moving batteries back and forth [from the appointment rooms to the lab] for testing can be **inefficient and cause delays**. These challenges are further compounded during home visits.

The scope paints a pretty clear picture of what their issues are. I can summarize it as: **batteries fail often and need to be tested, but testing with what they currently have is slow and laborious.** So, it makes sense then that the initial project scope states that they want a mobile approach to battery that can take the form of a new device or some sort of diagnostic toolkit, to help improve the efficiency of the process.

Now admittedly, as an engineer, you should never blindly trust the customer to describe the problem they're dealing with; it is your responsibility to do your due diligence and research the problem yourself, compile your findings, and return to the customer with this information. I will note though, that since these problem scopes were pre-defined by the course administrators, I think it's reasonable to assume that the problem scopes were vetted by them and were *actual problems that the RCC was dealing with* (spoiler alert, not all of them were).

So, we went into our first Community Partner visit under this false assumption. Our Community Partner, Stephen Klatt, is a technologist at the RCC who works closely with batteries for all kinds of devices and equipment that they service. During this first meeting, we noted the following:

- They use a variety of batteries, but the primary ones we're concerned with are the **rechargeable, sealed lead acid batteries** used on powered wheelchairs and Power Wheels-branded ride-on toy cars.
- Said batteries are **stored in a big charging room** with smart plugs.
- Said batteries are tested for a couple of metrics, but **mainly voltage and load testing**, and it's **not that in-depth**.
- The primary cause of failure is actually **user failure**, where families who borrow this equipment **abuse the batteries** by not adhering to charging protocols.

Sounds reasonable. However, this session didn't fully answer all of our questions, especially pertaining to the mobility and alleged clunkiness of the current testing procedures. Our team decided to shoot him an email with a few follow-ups, from which we learned:

- The current device they use to test batteries is a **BK Precision 8540 150W DC Electronic Load Power Source**, a power supply that also has load testing capabilities.
- Having been at the RCC for only **5 months**, Stephen noted **7 failed batteries**, some of which were old stock from not having an existing battery maintenance schedule.
- It takes about **4 - 6 hours to test** all the batteries on site.
- They only have **24 - 29** of these lead acid batteries at any given time.
- They **don't keep logs on battery failures** as they're considered consumables and aren't tracked the same way over devices in their inventory are tracked.

This starts to peel back the layers of the onion. Firstly, we did some research on the load tester that they use. Below is the device in question:

![The BK Precision load tester they use.](/blog/ibehs-4p03-rcc-consultancy/8540.png)

The data sheet for this device states that it only weighs about 6 lbs, with dimensions that would make it a little thinner than two Nintendo Wiis stacked side-by-side. Here's an image from the data sheet[^2] itself:

![Data sheet screenshot from DigiKey website of the BK load tester.](/blog/ibehs-4p03-rcc-consultancy/datasheet.png)

So, the device itself is **not actually that heavy, nor is it that large**. It would already be sufficiently mobile to move back and forth between appointment rooms and their electronics lab. However, from the information we've gathered, that **isn't even something they do**: if a battery appears to be dead during an appointment, they walk the battery back to the lab and grab a new one.

From the number of batteries and the hours of testing, we estimated that it would take at best, 8 minutes and at worst, 15 minutes, to test each battery, not accounting for walking distance to and from the lab, and assuming one individual testing a battery. Testing the batteries themselves didn't seem like it'd consume so much time to warrant it an issue.

What's more pressing is that we learned that they **do not inventory and log their batteries like other devices**. Without proper tracking, staff at the RCC wouldn't actually know whether the batteries in storage are dead or working.

All of these points suggest that maybe it **isn't the device itself** that's causing their problems, but the **lack of a proactive management and inventory process**.

## So what's actually behind the mask?

With this in mind, we entered into our second Community Partner meeting with Stephen, hoping to put to rest our burning question: **was the mobility of the testing device actually a problem?** One of my teammates asked that directly and...

...Yep, it wasn't actually a problem, not directly anyway.

What Stephen told us was:

- They're **not maintaining their inventory at all**. When he needs a battery, he'll look around and **test at random** until he finds a working battery.
- The **tester does actually have to stay in the lab** as it's used on their test bench for other tasks, so they can't relocate it, not that they did to begin with.
- **No information is kept** about whether batteries were poorly maintained, with documentation being in the form of sticky notes with random voltages that don't dictate battery health.
- The way he tests batteries will differ from how another technologist might test a battery, so the **procedures for testing are not standardized**.
- He suspects that portability of the testing device could be a reason why they're neglecting the batteries, but he thinks a **formal testing system** would **better address** the root issue.
- Such a procedural change would be a **"huge benefit to the department."**

So in actuality, their problem was the **lack of standardized testing procedure**, not that the device used was too immobile for testing. This is my **first major gripe** about this course: some of the assigned problem statements were not actual problems the RCC had to deal with. This is an anecdotal recount, so take it with a grain of salt, but from the other groups I've talked to, many of them also encountered a similar situation to ours:

1. They were handed a problem statement.
2. They dug a little deeper with their assigned Community Partner.
3. They find out this was never an actual problem, or at best, a surface-level symptom of a root problem.

You would think that, given that this course is meant for students to interact with a real stakeholder, that we would be given real problems to address, but I guess not? But hey, at least we found out what the real one was.

### Problem solved, then! Unfortunately...

As consultants, our group wanted to recommend the most simple, straightforward solution: standardize the protocol, document it, and start inventorying the batteries. They could've done this simply by:

- Creating a shared cloud drive with procedure documents and inventory stored together.
- Used online documents to note down the protocol, making them easily accessible.
- Used a spreadsheet as battery inventory management.
- If they really wanted to, use macros to calculate important metrics. Hell, they can even use basic linear regressions for battery health forecasting.

This would've solved the tracking issue, cost $0, and requires no complex maintenance.

**However...**

This is where the academic nature of the project clashed with real-world consulting. Although our problem pivot was well-received, our Project Managers (course TAs) noted a critical issue: you can't exactly perform an economic analysis on something that's free, and a plain spreadsheet isn't really *"innovative"* enough.

This is my **second major gripe** with this course: our initial solution was completely sound, but because it didn't fit within the course requirements, we couldn't suggest it to the RCC in its current form. That being said, I do understand the difficulty in involving imperfect, real-world stakeholders within a course framework assessing theoretical knowledge. However, it shouldn't come at the cost of compromising a realistic solution to a real stakeholder.

Again, this is an anecdotal recount, so take it with a grain of salt, but from my experience speaking with other groups, we were not alone in this situation. Many groups had relatively straightforward solutions that addressed their key issues, such as a floor plan for a rennovated waiting room, or hiring extra staff to solve a manpower issue. These groups however, like ours, were not allowed to approach the RCC with these solutions because they didn't cost money, were not "innovative" enough, or didn't fit within some other course requirement, and arguably to the detriment of the RCC.

**Why are we working with a real stakeholder if the course requirements force us to create pipe dream solutions?** At that point, I feel like the course should've just invented fictional companies for us to work with; it genuinely sucks that we're feeding BS to a real company with actionable problems, let alone a non-profit helping children.

So what did we do with this solution then, to get it to work with the course requirements? Well...

### I guess we'll use a sledgehammer to crack a walnut

After some brainstorming, we noted that a *somewhat* "reasonable" way to add some heft to the project would be to embody the role of a modern senior project manager at Microsoft and...

*...stuff artificial intelligence into the product.*

We opted to convert the simple spreadsheet into a **full-stack, AI-integrated, battery inventory management web application**. The app would centralize their data and testing procedures, but go a step beyond and automate the testing schedules, and predict battery health based on historical data points. This isn't actually as far-fetched as it seems and has some precedent already:

- Neural networks and their derivatives excel at capturing non-linear trends compared to traditional, physics-driven models. In fact, the National Renewable Energy Laboratory (NREL) in the United States has been *studying these models for a while*[^3].
- AI is already used extensively in inventory management for anomaly detection, forecasting, and replenishment predictions. There's a couple articles from IBM[^4] and McKinsey[^5] noting this.

So, it actually did make *some* sense to integrate AI into this process improvement solution, since such a model could be used to better predict battery health and streamline their testing operations.

Was it *"innovative?"*

I'd say so, yes.

Did it cost more money than "free?"

**Absolutely.**

Was it necessary?

**Absolutely not.**

Was it realistic?

**Absolutely not.**

Simply put, there was no way that a small non-profit was ever going to develop a full-stack AI application in house. No one at the RCC knew how to develop these sorts of software applications, no one at the RCC had the know-how to maintain such an application, and the costs of creating such a tool do not justify its "innovative" nature. Like I've said previously, this problem could've been addressed with a Google Sheet. Instead, we took a viable idea and hamstrung it with the weight of AI development.

The next section shows just how unrealistic it was.

## Alright, what's the damage?

As part of the project requirements, we had to compare a primary and alternative action plan through a variety of methods. I focused primarily on the economic analysis of the two plans we had in mind:

1. **In-House Development:** The RCC uses existing staff to build and maintain the system.
2. **Outsourced Agile Team:** The RCC contracts a third-party firm for the build, retaining only maintenance duties.

I opted to conduct the economic analysis using Net Present Value (NPV) as it would be the simplest way to compare the two projects: we just have to calculate the net receipts and disbursements, then bring them to the present. The formula I used was the following:

$$
\text{NPV} = \sum_{i=0}^N \frac{F_i}{(1+i)^n}
$$

where:

- $$i$$ is the compounding period.
- $$N$$ is the total number of periods.
- $$F_i$$ is the net future cash flow at that period.
- $$n$$ is the interest rate per period (calculated by dividing the MARR over $$N$$).

To conduct this economic analysis, I had to make a lot of assumptions, many of which were quite unrealistic, but I didn't really have another choice. I donâ€™t want to include all of them here since itâ€™d take up a lot of unnecessary space but to put it simply:

|**Primary Action Plan**|**Alternative Action Plan**|**Both**|
|--|--|--|
|Theyâ€™d fine-tune a pre-trained, time-series model, *Chronos-2*[^6].|Itâ€™d be a team in Canada, with salaries roughly estimated from Canadian averages.|Theyâ€™d use Amazon Web Services (AWS) for their cloud infrastructure, since itâ€™s the provider with the most market share as of writing this.|
|Made heavy assumptions on the task lengths based on professional judgement, to determine the cost of development, trying to account for work being done in employee off-time.|The team would require a technical project manager, UX designer, full-stack developer, and machine-learning engineer.|Made very conservative estimates on app usage, usage growth, and data set growth.|
|The fine-tuned model would be sufficient out-of-the-box as they began data collection.|Assume one year of app development, with another year of data collection and model design, training, and deployment.|We would project four years into the future, which gives at least two years of active use in both projects.|
|Someone at the RCC actually knows full-stack development.|The RCC would take out a CSBFP loan to bankroll the operation (as the initial seed grant of CA$3k is not even remotely enough).|We used an MARR of 8%, which was given to us in the project details.|
|The RCC has the IT support required to maintain such a system.|The loan would be paid back in monthly installments over ten years, right at the first month, without considering how theyâ€™d actually find the money to do it.|I quantified the time saved as an annuity based on estimated hourly savings and the average technologist salary in Manitoba.|

With this, hereâ€™s the cash flow for the Primary Action Plan. The NPV for this plan was **CA$27,153.50** (lol, lmao even).

![Cash flow diagram for the Primary Action Plan over four years](/blog/ibehs-4p03-rcc-consultancy/primaction.png)

Hereâ€™s the cash flow for the Alternative Action Plan, with an NPV of **-CA$179,939,59**.

![Cash flow diagram for the Primary Action Plan over four years](/blog/ibehs-4p03-rcc-consultancy/altaction.png)

![funny meme haha](/blog/ibehs-4p03-rcc-consultancy/inthered.jpg)

On paper, **In-House Development** was the clear winner as it showed a positive return by avoiding the massive upfront capital expenditure of hiring an external firm. The math tells the RCC that they should build this AI app themselves.

But spreadsheets do not tell the whole story.

## We have to cash in the reality check

The math suggests that the RCC should *immediately* begin developing an in-house, AI-augmented battery management system. The NPV is positive, the returns are reasonable, and the "innovation" box is checked.

But as I've stated numerous times already, **the RCC would not be able to build a solution like this, whatsoever.**

The biggest shortcoming of the Primary Action Plan economic analysis is that it relies heavily on the **assumption that the RCC possesses the "development prowess"** required to build and maintain a complicated, full-stack architecture like this one. A small non-profit, without a dedicated, internal development team, nor the supporting teams (like DevOps, IT, etc.) required to maintain such a system, would surely be unable to feasibly implement this plan.

There are other limitations to the economic analysis as well:

- We assumed that a pre-trained model would be sufficient for their use case so long as we trained it on open-source data, but the model would **likely be incompatible** with the batteries they have and would be **massively inaccurate** until enough data is collected for further refinement.
- We also only quantified the amount of time saved given that the app is built well and functions as intended. We **never account** for the **overhead of maintaining** the app, nor the development, as monetary quantities.
- We also had to make many assumptions on cloud service usage. From my personal experience, I can attest that this app would probably end up staying within free usage tiers of most services for its foreseeable lifespan, but we **donâ€™t know how large the RCC would scale** in the future.
- The Alternative Action Plan used averaged Canadian salaries for the identified, critical Agile team members, but outsourcing of development is typically done through **development firms** who will **have their own rates**, potentially higher than the national average. It also doesnâ€™t account for **any other fees** that would be tacked on to the whole project.
- Finally, the Alternative Action Plan assumes that the RCC would take out a $450,000 loan **without any feasible repayment plan**. As we all know, the Iron Bank will have its due[^7].

### The final recommendation

The one saving grace of this course is that we were not forced to suggest either of the proposed plans and we could recommend that they do nothing. Given how horrificly unreasonable the final Primary Action Plan turned out, we took the liberty of **not suggesting either plan**. We instead, recommended that they independently look into process improvements, particularly:

- Formalization of the testing protocol.
- Implementation of a simple spreadsheet inventory.

Or, to put it more bluntly, we told them "you donâ€™t need to spend $3k nor $450k. You just need a Google Sheet. We can't tell you that directly, though."

## Some closing remarks

As annoyed as I was with the course, it did highlight a very distinct tension between engineering education and engineering practice. Academia wants to ensure us engineers are equipped with the theoretical skills required to evaluate real-world projects, but **real-world projects are seldom as simple** as following a set of criteria.

Had we followed the course structure without critical thought, we wouldâ€™ve ended up pitching a white elephant to the RCC simply because it satisfied a grade rubric. **But it doesn't really take more than two brain cells to know that the compromise to our original solution was doomed from the start.**

If the course structure did allow for "simple" solutions, we likely wouldâ€™ve directly told them to implement the spreadsheet solution within the semester. That wouldâ€™ve been interesting to see, since it couldâ€™ve provided us with the opportunity to test and refine that protocol in the real world and measure real time savings, rather than forecasting hypothetical ones in a cash flow. But alas, it is but a dream.

All in all, this course was uniquely disappointing; it wasn't like traditionally disliked courses where their reputations come from notoriously bad professors or difficult-to-learn content, it was bad because it gave me a glimmer of hope that I could've enacted some real change for real people, only for said change to be snatched away in the name of pushing student "innovation." I'm not saying that academia should be pushing out a bunch of uncreative zombies left and right, but you can only re-invent the wheel so many times, and I really dislike the idea of selling dump truck drive hubs to the guy that just needs a new bicycle tyre.

Welp, fingers crossed that my capstone doesn't turn out this way.

[^1]: You can find out more about them [here](https://rccinc.ca/).
[^2]: The data sheet was found off of [DigiKey](https://www.digikey.ca/en/products/detail/b-k-precision/8540/1936377).
[^3]: Here's the [media relations article](https://www.nrel.gov/news/detail/program/2025/artificial-intelligence-models-improve-efficiency-of-battery-diagnostics) from them. It's actually pretty interesting, their use of a surrogate DNN model for health predictions.
[^4]: [Here's IBM](https://www.ibm.com/think/topics/ai-inventory-management) outlining how AI is used in numerous inventory mangement situations.
[^5]: [Here's McKinsey's article](https://www.mckinsey.com/industries/industrials-and-electronics/our-insights/distribution-blog/harnessing-the-power-of-ai-in-distribution-operations) on AI inventory management.
[^6]: SOTA multivariate time-series forecasting model from Amazon. You can find out more about it on their [Hugging Face model card page](https://huggingface.co/amazon/chronos-2).
[^7]: I didn't actually watch [Game of Thrones](https://www.youtube.com/watch?v=-9Rya1JlUac) but I think I've watched enough clips to get a gist ðŸ™ƒ.
